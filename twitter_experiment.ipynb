{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_constrained_optimization as tfco\n",
    "\n",
    "from utils import read_twitter_data, save_logs, load_logs, save_embeddings, load_embeddings, is_available\n",
    "from embeddings import get_bert_embeddings\n",
    "from model import get_dense_model\n",
    "from train import train_model, create_tensors\n",
    "from metrics import error_rate, group_false_positive_rates, f1, false_negative_equality_diff, false_positive_equality_diff\n",
    "from evaluation import eval_report, mcnemar_test\n",
    "from plot import plot_perf\n",
    "from configs import config as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_twitter_data()\n",
    "\n",
    "print(\"Overall toxicity proportion = {0:.2f}%\".format(data['target'].mean() * 100))\n",
    "for i in cf.identity_keys_twitter:\n",
    "    print(\"\\t{} proportion = {:.2f}% | toxicity proportion in {} = {:.2f}%\".format(i, data[i].mean()*100, i, data[data[i]]['target'].mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_available(cf.twitter_embeddings_path):\n",
    "    sentence_embeddings = load_embeddings(dataset='twitter')\n",
    "else:\n",
    "    sentence_embeddings = get_bert_embeddings(data['comment'])\n",
    "    save_embeddings(sentence_embeddings, dataset='twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_test_df = train_test_split(data, train_size=cf.train_size, random_state=cf.random_state, shuffle=True)\n",
    "val_df, test_df = train_test_split(val_test_df, train_size=cf.val_test_ratio, random_state=cf.random_state, shuffle=True)\n",
    "\n",
    "train_labels = np.array(train_df['target']).reshape(-1, 1).astype(float)\n",
    "val_labels = np.array(val_df['target']).reshape(-1, 1).astype(float)\n",
    "test_labels = np.array(test_df['target']).reshape(-1, 1).astype(float)\n",
    "\n",
    "train_groups = np.array(train_df[cf.identity_keys_twitter]).astype(int)\n",
    "val_groups = np.array(val_df[cf.identity_keys_twitter]).astype(int)\n",
    "test_groups = np.array(test_df[cf.identity_keys_twitter]).astype(int)\n",
    "\n",
    "train_relevant_obs_indices = np.where(train_df[cf.identity_keys_twitter].sum(axis=1))[0]\n",
    "\n",
    "train, val_test = train_test_split(sentence_embeddings, train_size=cf.train_size, random_state=cf.random_state, shuffle=True)\n",
    "val, test = train_test_split(val_test, train_size=cf.val_test_ratio, random_state=cf.random_state, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train plain (baseline / unconstrained) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_dense_model()\n",
    "plain_model = train_model(model, train, train_labels, val, val_labels, cf.twitter_plain_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate plain (unconstrained) model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model = get_dense_model()\n",
    "plain_model.load_weights('{}/{}.h5'.format(cf.MODELS_DIR, cf.twitter_plain_model_name))\n",
    "test_preds_plain = plain_model.predict_classes(test, batch_size=cf.hyperparams['batch_size'])\n",
    "test_probs_plain = plain_model.predict(test, batch_size=cf.hyperparams['batch_size'])\n",
    "\n",
    "eval_report(test_labels, test_preds_plain, test_probs_plain, test_groups)\n",
    "plot_perf(test_labels, test_preds_plain, test_groups, cf.identity_keys_twitter, 'Plain model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train fairness constrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(feat_tensor, feat_tensor_group, label_tensor, label_tensor_group, group_tensor) = create_tensors(cf.num_identities_twitter)\n",
    "\n",
    "constrained_model = get_dense_model()\n",
    "\n",
    "def predictions():\n",
    "  return constrained_model(feat_tensor)\n",
    "\n",
    "def predictions_group():\n",
    "  return constrained_model(feat_tensor_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define separate contexts for overall training data and groups of interest\n",
    "context = tfco.rate_context(predictions, lambda: label_tensor)\n",
    "context_group = tfco.rate_context(predictions_group, lambda: label_tensor_group)\n",
    "\n",
    "# define the objective = minimize negative of f1 score\n",
    "objective = -1 * tfco.f_score(context)\n",
    "\n",
    "# list group-specific FNRs/FPRs\n",
    "fnrs = []\n",
    "fprs = []\n",
    "constraints = []\n",
    "for iden in range(cf.num_identities_twitter):\n",
    "    context_group_subset = context_group.subset(lambda kk=iden: group_tensor[:, kk] > 0)\n",
    "    fnrs.append(tfco.false_negative_rate(context_group_subset))\n",
    "    fprs.append(tfco.false_positive_rate(context_group_subset))\n",
    "\n",
    "# define lower and upper bound constraints (see equation 3 in paper)\n",
    "constraints.append(tfco.upper_bound(fnrs) - tfco.false_negative_rate(context) <= cf.twitter_allowed_fnr_deviation)\n",
    "constraints.append(tfco.upper_bound(fprs) - tfco.false_positive_rate(context) <= cf.twitter_allowed_fpr_deviation)\n",
    "constraints.append(tfco.false_negative_rate(context) - tfco.lower_bound(fnrs) <= cf.twitter_allowed_fnr_deviation)\n",
    "constraints.append(tfco.false_positive_rate(context) - tfco.lower_bound(fprs) <= cf.twitter_allowed_fpr_deviation)\n",
    "\n",
    "# define problem, optimizer and variables to optimize\n",
    "problem = tfco.RateMinimizationProblem(objective, constraints)\n",
    "optimizer = tfco.ProxyLagrangianOptimizerV2(\n",
    "    optimizer=tf.keras.optimizers.Adam(cf.hyperparams['lr']),\n",
    "    constraint_optimizer=tf.keras.optimizers.Adam(cf.hyperparams['lr_constraints']),\n",
    "    num_constraints=problem.num_constraints)\n",
    "var_list = (constrained_model.trainable_weights + problem.trainable_variables + optimizer.trainable_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = train.shape[0]\n",
    "num_obs_sen = train_relevant_obs_indices.shape[0]\n",
    "\n",
    "# define checkpoint frequency\n",
    "num_steps = int(num_obs / cf.hyperparams['batch_size'])\n",
    "skip_steps = int(num_steps / 3)\n",
    "\n",
    "# list of recorded objectives and constraint violations for validation set\n",
    "error_list = []\n",
    "f1_list = []\n",
    "fped_list = []\n",
    "fned_list = []\n",
    "violations_list = []\n",
    "\n",
    "start_time = time.time()\n",
    "model_counter = 0\n",
    "for ep in range(cf.hyperparams['epochs']):  # loop over epochs\n",
    "    perm = np.random.permutation(train.shape[0]) # shuffle data\n",
    "    train, train_labels = train[perm], train_labels[perm]\n",
    "    for batch_index in range(num_steps):  # loop over minibatches\n",
    "        # training data indices of overall stream\n",
    "        batch_indices = np.arange(batch_index * cf.hyperparams['batch_size'], (batch_index + 1) * cf.hyperparams['batch_size'])\n",
    "        batch_indices = [ind % num_obs for ind in batch_indices]\n",
    "\n",
    "        # training data indices of group stream\n",
    "        batch_indices_group = np.arange(batch_index * cf.hyperparams['batch_size'], (batch_index + 1) * cf.hyperparams['batch_size'])\n",
    "        batch_indices_group = [train_relevant_obs_indices[ind % num_obs_sen] for ind in batch_indices_group]\n",
    "\n",
    "        # assign training data features, labels, groups from the minibatches to the respective tensors\n",
    "        feat_tensor.assign(train[batch_indices, :])\n",
    "        label_tensor.assign(train_labels[batch_indices])\n",
    "        feat_tensor_group.assign(train[batch_indices_group, :])\n",
    "        label_tensor_group.assign(train_labels[batch_indices_group])\n",
    "        group_tensor.assign(train_groups[batch_indices_group, :])\n",
    "\n",
    "        # gradient update\n",
    "        optimizer.minimize(problem, var_list=var_list)\n",
    "\n",
    "        # snapshot model parameters, evaluate objective and constraint violations on validation set\n",
    "        if batch_index % skip_steps == 0:\n",
    "\n",
    "            val_scores = constrained_model.predict_classes(val)\n",
    "\n",
    "            fped_list.append(false_positive_equality_diff(val_labels, val_scores, val_groups))\n",
    "            fned_list.append(false_negative_equality_diff(val_labels, val_scores, val_groups))\n",
    "            violations_list.append(fped_list[-1] + fned_list[-1])\n",
    "            error_list.append(error_rate(val_labels, val_scores))\n",
    "            f1_list.append(f1(val_labels, val_scores))\n",
    "\n",
    "            # save model weights\n",
    "            constrained_model.save_weights('{}/{}_{}.h5'.format(cf.MODELS_DIR, cf.twitter_constrained_model_name, model_counter))\n",
    "            model_counter += 1\n",
    "\n",
    "        # display most recently recorded objective and constraint violation for validation set\n",
    "        elapsed_time = time.time() - start_time\n",
    "        sys.stdout.write(\n",
    "            '\\rEpoch {}/{} | iter {}/{} | total elapsed time = {:.0f} secs | current error rate (val) = {:.4f} | current f1 (val) = {:.4f} | current total bias (val) = {:.4f}'.format(\n",
    "            ep + 1, cf.hyperparams['epochs'], batch_index + 1, num_steps, elapsed_time, error_list[-1], f1_list[-1], violations_list[-1]))\n",
    "print('\\ņTraining finalized.')\n",
    "save_logs(error_list, fped_list, fned_list, f1_list, cf.twitter_log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate discovered solutions interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that there may be several solutions that satisfy the constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "error_list, fped_list, fned_list, f1_list = load_logs(cf.twitter_log_name)\n",
    "violations_list = fped_list + fned_list\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(violations_list)), y=violations_list, mode='lines+markers', name='bias'))\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(f1_list)), y=f1_list, mode='lines+markers', name='f1'))\n",
    "fig.show()\n",
    "\n",
    "nonzero_f1 = np.where(f1_list==0, np.inf, f1_list)\n",
    "nonzero_bias = np.where(violations_list==0, np.inf, violations_list)\n",
    "print('Min bias = {:.2f} is at index {}. f1_score of that model is {:.3f} (warning: first iterations may give very low bias whole having very low f1 score)'.format(100 * min(nonzero_bias), np.argmin(nonzero_bias), f1_list[np.argmin(nonzero_bias)]))\n",
    "print('Max f1 = {:.3f} is at index {}. Bias of that model is {:.2f}.'.format(max(f1_list), np.argmax(f1_list), 100 * violations_list[np.argmax(f1_list)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fairness constrained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_index = 130  # select one of the discovered models\n",
    "constrained_model = get_dense_model()\n",
    "constrained_model.load_weights('{}/{}_{}.h5'.format(cf.MODELS_DIR, cf.twitter_constrained_model_name, selected_index))\n",
    "\n",
    "test_preds_const = constrained_model.predict_classes(test, batch_size=cf.hyperparams['batch_size'])\n",
    "test_probs_const = constrained_model.predict(test, batch_size=cf.hyperparams['batch_size'])\n",
    "\n",
    "eval_report(test_labels, test_preds_const, test_probs_const, test_groups)\n",
    "plot_perf(test_labels, test_preds_const, test_groups, cf.identity_keys_twitter, 'Constrained model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare unconstrained vs. fairness constrained model statistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcnemar_test(labels=test_labels.ravel(), model1_preds=test_preds_plain.ravel(), model1_name='Baseline model', \n",
    "                                         model2_preds=test_preds_const.ravel(), model2_name='Constrained model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
